{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal</th>\n",
       "      <th>hair</th>\n",
       "      <th>feathers</th>\n",
       "      <th>eggs</th>\n",
       "      <th>milk</th>\n",
       "      <th>airborne</th>\n",
       "      <th>aquatic</th>\n",
       "      <th>predator</th>\n",
       "      <th>toothed</th>\n",
       "      <th>backbone</th>\n",
       "      <th>breathes</th>\n",
       "      <th>venomous</th>\n",
       "      <th>fins</th>\n",
       "      <th>legs</th>\n",
       "      <th>tail</th>\n",
       "      <th>domestic</th>\n",
       "      <th>catsize</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>mammal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antelope</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>mammal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bass</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>fish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bear</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>mammal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boar</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>mammal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     animal   hair  feathers   eggs   milk  airborne  aquatic  predator  \\\n",
       "0  aardvark   True     False  False   True     False    False      True   \n",
       "1  antelope   True     False  False   True     False    False     False   \n",
       "2      bass  False     False   True  False     False     True      True   \n",
       "3      bear   True     False  False   True     False    False      True   \n",
       "4      boar   True     False  False   True     False    False      True   \n",
       "\n",
       "   toothed  backbone  breathes  venomous   fins  legs   tail  domestic  \\\n",
       "0     True      True      True     False  False     4  False     False   \n",
       "1     True      True      True     False  False     4   True     False   \n",
       "2     True      True     False     False   True     0   True     False   \n",
       "3     True      True      True     False  False     4  False     False   \n",
       "4     True      True      True     False  False     4   True     False   \n",
       "\n",
       "   catsize    type  \n",
       "0     True  mammal  \n",
       "1     True  mammal  \n",
       "2    False    fish  \n",
       "3     True  mammal  \n",
       "4     True  mammal  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('S:\\ML\\Datasets\\Classification\\zoo.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101 entries, 0 to 100\n",
      "Data columns (total 18 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   animal    101 non-null    object\n",
      " 1   hair      101 non-null    bool  \n",
      " 2   feathers  101 non-null    bool  \n",
      " 3   eggs      101 non-null    bool  \n",
      " 4   milk      101 non-null    bool  \n",
      " 5   airborne  101 non-null    bool  \n",
      " 6   aquatic   101 non-null    bool  \n",
      " 7   predator  101 non-null    bool  \n",
      " 8   toothed   101 non-null    bool  \n",
      " 9   backbone  101 non-null    bool  \n",
      " 10  breathes  101 non-null    bool  \n",
      " 11  venomous  101 non-null    bool  \n",
      " 12  fins      101 non-null    bool  \n",
      " 13  legs      101 non-null    int64 \n",
      " 14  tail      101 non-null    bool  \n",
      " 15  domestic  101 non-null    bool  \n",
      " 16  catsize   101 non-null    bool  \n",
      " 17  type      101 non-null    object\n",
      "dtypes: bool(15), int64(1), object(2)\n",
      "memory usage: 4.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "animal      object\n",
       "hair          bool\n",
       "feathers      bool\n",
       "eggs          bool\n",
       "milk          bool\n",
       "airborne      bool\n",
       "aquatic       bool\n",
       "predator      bool\n",
       "toothed       bool\n",
       "backbone      bool\n",
       "breathes      bool\n",
       "venomous      bool\n",
       "fins          bool\n",
       "legs         int64\n",
       "tail          bool\n",
       "domestic      bool\n",
       "catsize       bool\n",
       "type        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "animal      0\n",
       "hair        0\n",
       "feathers    0\n",
       "eggs        0\n",
       "milk        0\n",
       "airborne    0\n",
       "aquatic     0\n",
       "predator    0\n",
       "toothed     0\n",
       "backbone    0\n",
       "breathes    0\n",
       "venomous    0\n",
       "fins        0\n",
       "legs        0\n",
       "tail        0\n",
       "domestic    0\n",
       "catsize     0\n",
       "type        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aardvark', 'antelope', 'bass', 'bear', 'boar', 'buffalo', 'calf',\n",
       "       'carp', 'catfish', 'cavy', 'cheetah', 'chicken', 'chub', 'clam',\n",
       "       'crab', 'crayfish', 'crow', 'deer', 'dogfish', 'dolphin', 'dove',\n",
       "       'duck', 'elephant', 'flamingo', 'flea', 'frog', 'fruitbat',\n",
       "       'giraffe', 'girl', 'gnat', 'goat', 'gorilla', 'gull', 'haddock',\n",
       "       'hamster', 'hare', 'hawk', 'herring', 'honeybee', 'housefly',\n",
       "       'kiwi', 'ladybird', 'lark', 'leopard', 'lion', 'lobster', 'lynx',\n",
       "       'mink', 'mole', 'mongoose', 'moth', 'newt', 'octopus', 'opossum',\n",
       "       'oryx', 'ostrich', 'parakeet', 'penguin', 'pheasant', 'pike',\n",
       "       'piranha', 'pitviper', 'platypus', 'polecat', 'pony', 'porpoise',\n",
       "       'puma', 'pussycat', 'raccoon', 'reindeer', 'rhea', 'scorpion',\n",
       "       'seahorse', 'seal', 'sealion', 'seasnake', 'seawasp', 'skimmer',\n",
       "       'skua', 'slowworm', 'slug', 'sole', 'sparrow', 'squirrel',\n",
       "       'starfish', 'stingray', 'swan', 'termite', 'toad', 'tortoise',\n",
       "       'tuatara', 'tuna', 'vampire', 'vole', 'vulture', 'wallaby', 'wasp',\n",
       "       'wolf', 'worm', 'wren'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['animal'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mammal          41\n",
       "bird            20\n",
       "fish            13\n",
       "invertebrate    10\n",
       "insect           8\n",
       "reptile          5\n",
       "amphibian        4\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x221ad0d8a90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "sns.catplot(data=df,x=\"type\",kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['animal', 'hair', 'feathers', 'eggs', 'milk', 'airborne', 'aquatic',\n",
       "       'predator', 'toothed', 'backbone', 'breathes', 'venomous', 'fins',\n",
       "       'legs', 'tail', 'domestic', 'catsize', 'type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal</th>\n",
       "      <th>hair</th>\n",
       "      <th>feathers</th>\n",
       "      <th>eggs</th>\n",
       "      <th>milk</th>\n",
       "      <th>airborne</th>\n",
       "      <th>aquatic</th>\n",
       "      <th>predator</th>\n",
       "      <th>toothed</th>\n",
       "      <th>backbone</th>\n",
       "      <th>breathes</th>\n",
       "      <th>venomous</th>\n",
       "      <th>fins</th>\n",
       "      <th>legs</th>\n",
       "      <th>tail</th>\n",
       "      <th>domestic</th>\n",
       "      <th>catsize</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mammal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antelope</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mammal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bass</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bear</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mammal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boar</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mammal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     animal  hair  feathers  eggs  milk  airborne  aquatic  predator  toothed  \\\n",
       "0  aardvark     1         0     0     1         0        0         1        1   \n",
       "1  antelope     1         0     0     1         0        0         0        1   \n",
       "2      bass     0         0     1     0         0        1         1        1   \n",
       "3      bear     1         0     0     1         0        0         1        1   \n",
       "4      boar     1         0     0     1         0        0         1        1   \n",
       "\n",
       "   backbone  breathes  venomous  fins  legs  tail  domestic  catsize    type  \n",
       "0         1         1         0     0     4     0         0        1  mammal  \n",
       "1         1         1         0     0     4     1         0        1  mammal  \n",
       "2         1         0         0     1     0     1         0        0    fish  \n",
       "3         1         1         0     0     4     0         0        1  mammal  \n",
       "4         1         1         0     0     4     1         0        1  mammal  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "df[['hair', 'feathers', 'eggs', 'milk', 'airborne', 'aquatic','predator', 'toothed', 'backbone', 'breathes', \n",
    "    'venomous', 'fins', 'tail', 'domestic', 'catsize']]=df[['hair', 'feathers', 'eggs', 'milk', 'airborne', 'aquatic','predator', 'toothed', 'backbone', 'breathes', \n",
    "    'venomous', 'fins', 'tail', 'domestic', 'catsize']].apply(le.fit_transform)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal</th>\n",
       "      <th>hair</th>\n",
       "      <th>feathers</th>\n",
       "      <th>eggs</th>\n",
       "      <th>milk</th>\n",
       "      <th>airborne</th>\n",
       "      <th>aquatic</th>\n",
       "      <th>predator</th>\n",
       "      <th>toothed</th>\n",
       "      <th>backbone</th>\n",
       "      <th>breathes</th>\n",
       "      <th>venomous</th>\n",
       "      <th>fins</th>\n",
       "      <th>legs</th>\n",
       "      <th>tail</th>\n",
       "      <th>domestic</th>\n",
       "      <th>catsize</th>\n",
       "      <th>type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mammal</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antelope</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mammal</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bass</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bear</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mammal</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boar</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mammal</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>wallaby</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mammal</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>wasp</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>insect</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>wolf</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mammal</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>worm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>invertebrate</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>wren</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bird</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       animal  hair  feathers  eggs  milk  airborne  aquatic  predator  \\\n",
       "0    aardvark     1         0     0     1         0        0         1   \n",
       "1    antelope     1         0     0     1         0        0         0   \n",
       "2        bass     0         0     1     0         0        1         1   \n",
       "3        bear     1         0     0     1         0        0         1   \n",
       "4        boar     1         0     0     1         0        0         1   \n",
       "..        ...   ...       ...   ...   ...       ...      ...       ...   \n",
       "96    wallaby     1         0     0     1         0        0         0   \n",
       "97       wasp     1         0     1     0         1        0         0   \n",
       "98       wolf     1         0     0     1         0        0         1   \n",
       "99       worm     0         0     1     0         0        0         0   \n",
       "100      wren     0         1     1     0         1        0         0   \n",
       "\n",
       "     toothed  backbone  breathes  venomous  fins  legs  tail  domestic  \\\n",
       "0          1         1         1         0     0     4     0         0   \n",
       "1          1         1         1         0     0     4     1         0   \n",
       "2          1         1         0         0     1     0     1         0   \n",
       "3          1         1         1         0     0     4     0         0   \n",
       "4          1         1         1         0     0     4     1         0   \n",
       "..       ...       ...       ...       ...   ...   ...   ...       ...   \n",
       "96         1         1         1         0     0     2     1         0   \n",
       "97         0         0         1         1     0     6     0         0   \n",
       "98         1         1         1         0     0     4     1         0   \n",
       "99         0         0         1         0     0     0     0         0   \n",
       "100        0         1         1         0     0     2     1         0   \n",
       "\n",
       "     catsize          type  label  \n",
       "0          1        mammal      5  \n",
       "1          1        mammal      5  \n",
       "2          0          fish      2  \n",
       "3          1        mammal      5  \n",
       "4          1        mammal      5  \n",
       "..       ...           ...    ...  \n",
       "96         1        mammal      5  \n",
       "97         0        insect      3  \n",
       "98         1        mammal      5  \n",
       "99         0  invertebrate      4  \n",
       "100        0          bird      1  \n",
       "\n",
       "[101 rows x 19 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label']=le.fit_transform(df['type'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amphibian', 'bird', 'fish', 'insect', 'invertebrate', 'mammal',\n",
       "       'reptile'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "animal      object\n",
       "hair         int64\n",
       "feathers     int64\n",
       "eggs         int64\n",
       "milk         int64\n",
       "airborne     int64\n",
       "aquatic      int64\n",
       "predator     int64\n",
       "toothed      int64\n",
       "backbone     int64\n",
       "breathes     int64\n",
       "venomous     int64\n",
       "fins         int64\n",
       "legs         int64\n",
       "tail         int64\n",
       "domestic     int64\n",
       "catsize      int64\n",
       "type        object\n",
       "label        int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.boxplot(figsize=(14,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of LogisticClassifier for MultiClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(columns=['animal','type','label'])\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Splitting into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Model Training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression(multi_class='multinomial')\n",
    "model.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Testing\n",
    "ypred=model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9523809523809523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       0.67      1.00      0.80         2\n",
      "           3       1.00      1.00      1.00         3\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       1.00      1.00      1.00        12\n",
      "           6       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.78      0.83      0.80        21\n",
      "weighted avg       0.92      0.95      0.93        21\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Model Evaluation\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "cm=confusion_matrix(ytest,ypred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "print('Accuracy:',accuracy_score(ytest,ypred))\n",
    "print(classification_report(ytest,ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning of logisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'class_weight': 'balanced', 'multi_class': 'auto', 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.9708333333333333\n"
     ]
    }
   ],
   "source": [
    "## Model\n",
    "model=LogisticRegression()\n",
    "\n",
    "## Parameters\n",
    "penalty=('l1', 'l2', 'elasticnet')\n",
    "C=[0.0001,0.001,0.01,0.1,1,10]\n",
    "class_weight=['balanced']\n",
    "solver=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "multi_class=['auto', 'ovr', 'multinomial']\n",
    "\n",
    "#grid\n",
    "grid=dict(penalty=penalty,C=C,class_weight=class_weight,solver=solver,multi_class=multi_class)\n",
    "\n",
    "##cv\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "cv=RepeatedStratifiedKFold(n_splits=5,n_repeats=3,random_state=42)\n",
    "\n",
    "## GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_cv=GridSearchCV(estimator=model,cv=cv,param_grid=grid,scoring='accuracy')\n",
    "\n",
    "##result\n",
    "res=grid_cv.fit(xtrain,ytrain)\n",
    "print(res.best_params_)\n",
    "print(res.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight='balanced', solver='liblinear')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Model Retraining\n",
    "model=LogisticRegression(C= 10, class_weight = 'balanced', multi_class = 'auto', penalty = 'l2', solver = 'liblinear')\n",
    "model.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Retesting\n",
    "ypred=model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9523809523809523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       0.67      1.00      0.80         2\n",
      "           3       1.00      1.00      1.00         3\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       1.00      1.00      1.00        12\n",
      "           6       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.78      0.83      0.80        21\n",
      "weighted avg       0.92      0.95      0.93        21\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD8CAYAAAA2Y2wxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaq0lEQVR4nO3dfZRV1Znn8e9zi1IQ34MK9dJdTkN3TGuUDtDJkLGxjWIb3lydQZ2Fmh5WmE5MAp1piU5cjWTajq7YRhwnGoJGTJSAGoYIJMH2pZGJyIsSA1U0iCBUUUJ6bNvXhKpbz/xRF1JC1b3nVp1zz90nv0/WWdx7invuLxvXw2afffY2d0dERJKTSzuAiEjWqdCKiCRMhVZEJGEqtCIiCVOhFRFJmAqtiEjCVGhFRPpgZg+Y2UEz29rj3DfNbLuZvWxmy83s1FLXUaEVEenbg8BlR517EjjX3T8K7ABuKnURFVoRkT64+1rgjaPOrXH3zsLb9UBDqesMSiDbB0xo+FRQj56tO9iSdgSR31mdh9psoNfo+NdXI9ec4874g/8GzOpxaqG7Lyzj6/4rsLTUb0q80IqIVKtCUS2nsB5hZl8DOoGHS/1eFVoRyZaufOJfYWbXAZOAiz3CgjEqtCKSLfnO0r9nAMzsMuCrwJ+5+3tRPqNCKyKZ4t4V27XMbAkwARhmZq3APLpnGRwPPGlmAOvd/a+LXUeFVkSypSu+QuvuV/dy+v5yr6NCKyLZEmOPNi4qtCKSLRW4GVYuFVoRyRb1aEVEkuUJzzroDxVaEcmWGG+GxUWFVkSyRUMHIiIJ080wEZGEVWGPNrhlEs8YcQbfWnYHi5+5n+89tYi/nHlF2pEimXjpBLZtXcv25nXMveH6tOOUFFpeUOZKCCJvvjP6USEWYT2EAYl7mcTTzzydD515Oju3vsKQoUNY+JN7uXnm3/Hazr2xXD+JZRJzuRwt257jssuvprW1nfXPr2bGNV+gpWVn7N8Vh9DygjJXQiXyxrFM4m9e/lnkmnP8RycO+PuiCK5H+8bBN9i59RUA3n/3fV7buZdhw4elnKq4cWNHs2vXHnbv3ktHRwfLlq1gyuSJacfqU2h5QZkrIZS87vnIR6WULLRm9mEz+6qZ3W1mCwqvz6lEuFKGN5zFqHNH0vLS9rSjFFVXP5x9rfuPvG9ta6eubniKiYoLLS8ocyUEk9e7oh8VUrTQmtlXgR8CBmwANhZeLzGzG4t8bpaZbTKzTfvfbYsz7xFDThjM/IXzuOeWb/PeO5FWKktNYYWfD0h6yGYgQssLylwJweTt6op+VEipWQczgT92946eJ83sTmAbcFtvH+q5ankSW9nUDKph/sJb+KflT/HcT9bFffnYtbW209hQd+R9Q/0I2tsPpJiouNDygjJXQjB5A5x10AXU9XJ+ROFnqZh7x9+y95XXePS7j6cVoSwbN21h5MizaWpqpLa2lunTp/LEyjVpx+pTaHlBmSshmLz5juhHhZTq0c4BnjKzncC+wrnfA0YCX0wwV5/OG3suEz9zCbtaXmXRz+4D4Lu3P8ALT29II04k+Xye2XNuZvWqR6jJ5Xhw8VKam3ekHatPoeUFZa6EYPJW4SO4Jad3mVkOGAfU0z0+2wps9Ii37LQLrohEFcf0rl8/vyRyzRn8iasrMr2r5JNh3r0vxPoKZBERGbgq7NHqEVwRyRYVWhGRZHkFb3JFpUIrItlShdO7VGhFJFs0dCAikjD1aEVEEqYerYhIwtSjFRFJWKd2wRURSVYV9miDW/hbRKSoGJdJNLMHzOygmW3tce50M3vSzHYWfj2t1HVUaEUkW+Jd+PtB4LKjzt0IPOXuo4CnCu+LUqEVkWyJsUfr7muBN446PRVYXHi9GJhW6jqJj9GGthrWJ8+sil16yhJaG4skKvkx2rPcvR3A3dvN7MxSH9DNMBHJljJmHZjZLGBWj1MLCzvExEqFVkSypYx9zHpuu1WGA2Y2otCbHQEcLPUBjdGKSLYkvznjj4HrCq+vA1aU+oB6tCKSLTE+gmtmS4AJwDAzawXm0b0p7TIzmwnsBf5zqeuo0IpItsR4M8zdr+7jRxeXcx0VWhHJlnyk7QwrSoVWRLJFq3eJiCRMhVZEJGFVuKiMCq2IZIp3RZ9HWykqtCKSLRo6EBFJmGYdiIgkTD1aEZGEVWGhDXKtg4mXTmDb1rVsb17H3BuuTztOSWeMOINvLbuDxc/cz/eeWsRfzrwi7UglhdbGoMyVEERe9+hHhQRXaHO5HHcvuJVJk2dw3vkXceWV0zjnnFFpxyoqn8/z7a/fx3UXzeQLU77EtOum8vujfi/tWH0KsY2VOXnB5E1+UZmyBVdox40dza5de9i9ey8dHR0sW7aCKZMnph2rqDcOvsHOra8A8P677/Pazr0MGz4s5VR9C7GNlTl5weTt8uhHhfS70JrZX8UZJKq6+uHsa91/5H1rWzt1dcPTiNIvwxvOYtS5I2l5aXvaUfoUYhsrc/KCyZvPRz8qZCA92vl9/cDMZpnZJjPb1NX17gC+otdrH3POKzjWMhBDThjM/IXzuOeWb/PeO++lHadPIbaxMicvlLze1RX5qJSisw7M7OW+fgSc1dfneq5aPui4+lj/JNpa22lsqDvyvqF+BO3tB+L8ikTUDKph/sJb+KflT/HcT9alHaeoENtYmZMXTN4qfDKsVI/2LOBaYHIvx/9LNlrvNm7awsiRZ9PU1EhtbS3Tp0/liZVr0ohSlrl3/C17X3mNR7/7eNpRSgqxjZU5ecHkjXe78ViUmke7EjjR3bcc/QMzezaJQKXk83lmz7mZ1aseoSaX48HFS2lu3pFGlMjOG3suEz9zCbtaXmXRz+4D4Lu3P8ALT29IOVnvQmxjZU5eMHmrsEdrSY+xxD10kDRtNy6Sns5DbccOBJfp3b+7KnLNGfr1Hw74+6LQk2Eiki1aJlFEJGFVOHSgQisimVLJaVtRqdCKSLaoRysikjAVWhGRhGnhbxGRZGnPMBGRpKnQiogkTLMOREQSVoU92uAW/hYRKSrGhb/N7G/MbJuZbTWzJWY2uD+RVGhFJFM83xX5KMbM6oEvA2Pc/VygBriqP5k0dHCUEBdoubbuE2lHKNtD+59PO4JkVbxDB4OAIWbWAZwA7C/x+3ulHq2IZIp3eeSj524whWPWkeu4twF3AHuBduDf3b1fC/CqRysi2VJGj7bnbjBHM7PTgKnA2cCbwKNmNsPdf1BuJPVoRSRbuso4ivsUsNvdf+XuHcCPgP/Yn0jq0YpIpnhnbPNo9wIfN7MTgPeBi4FN/bmQCq2IZEtMddbdXzCzx4AXgU7gJfoYZihFhVZEMiXOtQ7cfR4wb6DXUaEVkWypvidwVWhFJFu0epeISNLUoxURSZZ3pp3gWCq0IpIpVbjbuAqtiGSMCq2ISLLUoxURSVg1Ftog1zqYeOkEtm1dy/bmdcy94fq040QSUuZBx9dy8//5BvN/cgf/c823mPo309OOFElIbXxYaJlDyOt5i3xUirknO+ds0HH1sX5BLpejZdtzXHb51bS2trP++dXMuOYLtLTsjPNrYpV05iTWoz3+hMH85r1fUzOohpse+3semf8Ar74UXxvHvR6t/rtIXiXydh5qG3D1e/3CCZFrzvC1z1ak2pbs0ZrZh83sYjM78ajzlyUXq2/jxo5m16497N69l46ODpYtW8GUyRPTiBJZiJl/896vAagZVEPNoBqovjngHxBiG4eWOZS83mWRj0opWmjN7MvACuBLwFYzm9rjx/+QZLC+1NUPZ1/rbxc5b21rp65ueBpRIgsxs+Vy3LL6m9y1+X62rXuZV7dUZy/rsBDbOLTMoeT1ruhHpZS6GfY54GPu/o6ZNQGPmVmTuy8A+vzroLBK+SwAqzmFXG5oXHkxO/Zrkx7+GKgQM3tXF7dcfgNDTj6BL35nLvV/2Ejbjn1px+pTiG0cWuZQ8rpXrqcaVamhgxp3fwfA3fcAE4C/MLM7KVJo3X2hu49x9zFxFlmAttZ2GhvqjrxvqB9Be/uBWL8jbiFmPuz9t97jX9Zv49w/G512lKJCbOPQMoeStxp7tKUK7etmdsHhN4WiOwkYBpyXYK4+bdy0hZEjz6apqZHa2lqmT5/KEyv7tY1PxYSW+aTTT2bIyScAUHv8cXxk/Ed5fVdbyqmKC62NIbzMoeTtylvko1JKDR1cS/eCt0e4eydwrZl9J7FUReTzeWbPuZnVqx6hJpfjwcVLaW7ekUaUyELLfMqZpzHzH79ILpfDcsbGVT/nF09vTjtWUaG1MYSXOZS8lbzJFVVw07vkWNpuXLIijuldey64JHLNadryZEWqsp4ME5FMqcL7cyq0IpIt1Th0oEIrIplSjdO7VGhFJFPyFZxNEJUKrYhkinq0IiIJ0xitiEjCNOtARCRh6tGKiCQs31V9+xmo0IpIplTj0EH1lX4RkQHocot8lGJmp5rZY2a23cxazKxfz7urRysimRLz9K4FwE/d/TNmdhxwQn8uokIrIpkS19CBmZ0MXAh8tvu6fgg41J9rqdBmQIgrYTWeNCztCGXZ9/a/ph1BIooyJHBYz91gCha6+8LC6/8A/Ar4npmdD2wGZrv7u+Vm0hitiGRKvisX+ei5G0zhWNjjUoOAPwHudffRwLvAjf3JpEIrIpniZRwltAKt7v5C4f1jdBfesqnQikimxDXrwN1fB/aZ2R8VTl0MNPcnk8ZoRSRTYp518CXg4cKMg1eBv+rPRVRoRSRT4tzc1t23AGMGeh0VWhHJFEdrHYiIJKpT69GKiCRLPVoRkYTFOUYbFxVaEckU9WhFRBKmHq2ISMLy6tGKiCSrCneyCfMR3ImXTmDb1rVsb17H3BuuTztOJKFlDi3v7XfPZ+P2Z/jpusfTjlKW0No5hLxdWOSjUoIrtLlcjrsX3MqkyTM47/yLuPLKaZxzzqi0YxUVWubQ8gI8vmQFn53++bRjlCW0dg4lb4yLysSmZKE1s3FmNrbw+iNm9hUzuzz5aL0bN3Y0u3btYffuvXR0dLBs2QqmTJ6YVpxIQsscWl6ADc+/yJv/9lbaMcoSWjuHkrerjKNSihZaM5sH3A3ca2bfAO4BTgRuNLOvVSDfMerqh7Ovdf+R961t7dTVDU8jSmShZQ4tb6hCa+dQ8naZRT4qpdTNsM8AFwDHA68DDe7+lpl9E3gBuLW3D/VctdxqTiGXGxpbYOulcbwat73sIbTMoeUNVWjtHErefNoBelGq0Ha6ex54z8x2uftbAO7+vpn12fMurFK+EGDQcfWx/km0tbbT2FB35H1D/Qja2w/E+RWxCy1zaHlDFVo7h5I3xFkHh8zs8K6PHzt80sxOIaV5wRs3bWHkyLNpamqktraW6dOn8sTKNWlEiSy0zKHlDVVo7RxK3mqcdVCqR3uhu/8GwN17FtZa4LrEUhWRz+eZPedmVq96hJpcjgcXL6W5eUcaUSILLXNoeQEWLLyNj48fw2kfOpWf/3INd912L8seXp52rKJCa+dQ8lbfYAZY0mMscQ8dSDZoF1zpTeehtgF3Mx+qnxG55lzb9oOKdGv1ZJiIZIrWOhARSVi+Cm+GqdCKSKaoRysikjAVWhGRhFXhlmEqtCKSLerRiogkLMRHcEVEglKNj+Cq0IpIpmjoQEQkYdVYaIPbYUFEpJi4d1gwsxoze8nMVvY3k3q0IpIpCYzRzgZagJP7ewH1aEUkU/JlHKWYWQPwaWDRQDKpRyupCG01rPf3P5d2hLINqftPaUdIRVcZCyX23A2mYGFh44LD7gLmAicNJJMKrYhkSjk3w3ruBnM0M5sEHHT3zWY2YSCZVGhFJFNiXAB7PDClsOv3YOBkM/uBu88o90IaoxWRTIlru3F3v8ndG9y9CbgKeLo/RRbUoxWRjOm06tvURYVWRDIliTLr7s8Cz/b38yq0IpIp1fhkmAqtiGRKOdO7KkWFVkQypfrKrAqtiGSMhg5ERBKWr8I+rQqtiGSKerQiIglz9WhFRJJVjT3aIB/BnXjpBLZtXcv25nXMveH6tONEElrm0PJCGJlv/oc7ufDTVzFtxl8fOXfHPYuYfPXnuOLaz/Plm77OW2+/k2LC4kJo4y488lEpwRXaXC7H3QtuZdLkGZx3/kVceeU0zjlnVNqxigotc2h5IZzM0y6/hPvu/PsPnPvE2NEs//59LH/oXpoa61n0/aUppSsulDaOe4eFOARXaMeNHc2uXXvYvXsvHR0dLFu2gimTJ6Ydq6jQMoeWF8LJPOaC8zjl5A8ubTr+Tz/GoEE1AHz0jz/MgYPVuVZvKG3ciUc+KqXsQmtmDyURJKq6+uHsa91/5H1rWzt1dcNTTFRaaJlDywthZu7N8lVr+OQnxqYdo1ehtLGX8b9KKXozzMx+fPQp4CIzOxXA3af08bkjq5ZbzSnkckMHnvS31z7mnHv13WXsKbTMoeWFMDMf7TuLl1BTU8OkSy9KO0qvQmnjarwZVmrWQQPQTPd+OU53oR0D/GOxD/VctXzQcfWx/km0tbbT2FD324D1I2hvPxDnV8QutMyh5YUwM/e0YvWTrP2/G1h09zd6LWjVIJQ2rsbpXaWGDsYAm4GvAf9eWCrsfXf/Z3f/56TD9Wbjpi2MHHk2TU2N1NbWMn36VJ5YuSaNKJGFljm0vBBm5sPWrd/E/Q8/yv+6fR5DBg9OO06fQmnjuBb+jlPRHq27dwHfMrNHC78eKPWZpOXzeWbPuZnVqx6hJpfjwcVLaW7ekWakkkLLHFpeCCfzDfNuY+NLL/Pmm29x8bQZfGHmNSz6/lIOdXTwuTlfA7pviM2b+6WUkx4rlDbOV+FwhpUzxmJmnwbGu/v/iPqZuIcORNKgXXAro/NQ24DHTf7L718RueY88tryiozTlNU7dfdVwKqEsoiIDFg1jtHqEVwRyZQQZx2IiARFOyyIiCRMQwciIgmrxlkHKrQikikaOhARSZhuhomIJExjtCIiCdPQgYhIwqpxRbHgFv4WESkmj0c+ijGzRjN7xsxazGybmc3ubyb1aEUkU2IcOugE/ru7v2hmJwGbzexJd28u90IqtCKSKXENHbh7O9BeeP22mbUA9XSv0V0WFdqjNJ40LO0IZdv3dnXuMZUlIa6E9bsqiZthZtYEjAZe6M/nNUYrIplSzp5hZjbLzDb1OGYdfT0zOxF4HJjj7m/1J5N6tCKSKeU8gttz263emFkt3UX2YXf/UX8zqdCKSKbENXRg3Zu33Q+0uPudA7mWhg5EJFO68MhHCeOBa4A/N7MthePy/mRSj1ZEMiXGWQfr6N75e8BUaEUkU/QIrohIwrSojIhIwvJefQslqtCKSKZU46IyKrQikikaoxURSZjGaEVEEtaloQMRkWSpRysikrBqnHUQ5CO4Ey+dwLata9nevI65N1yfdpySbr97Phu3P8NP1z2edpTIQmtjUOZKCCFvl3vko1KCK7S5XI67F9zKpMkzOO/8i7jyymmcc86otGMV9fiSFXx2+ufTjhFZiG2szMkLJW85yyRWSlmF1sw+aWZfMbNLkwpUyrixo9m1aw+7d++lo6ODZctWMGXyxLTiRLLh+Rd589/6tYxlKkJsY2VOXih5g+vRmtmGHq8/B9wDnATMM7MbE87Wq7r64exr3X/kfWtbO3V1w9OIklkhtrEyJy+UvNXYoy11M6y2x+tZwCXu/iszuwNYD9zW24cKq5TPArCaU8jlhsaR9fC1jzlXjU+ChCzENlbm5IWSN+/5tCMco1ShzZnZaXT3fM3dfwXg7u+aWWdfH+q5avmg4+pj/ZNoa22nsaHuyPuG+hG0tx+I8yt+54XYxsqcvFDyVmPxLzVGewqwGdgEnG5mw+HIHjqxrNNYro2btjBy5Nk0NTVSW1vL9OlTeWLlmjSiZFaIbazMyQslb4wLf8emaI/W3Zv6+FEXcEXsaSLI5/PMnnMzq1c9Qk0ux4OLl9LcvCONKJEtWHgbHx8/htM+dCo//+Ua7rrtXpY9vDztWH0KsY2VOXmh5K3GHq0lHSruoYOkabtxkfR0Hmob8L+UR5z6kcg1p/3N5or8y1xPholIpugRXBGRhFXjI7gqtCKSKdU4RqtCKyKZomUSRUQSph6tiEjCtJWNiEjC1KMVEUmYZh2IiCRMN8NERBJWjUMHwe2wICJSTJzr0ZrZZWb2L2b2ykDW4FaPVkQyJa4erZnVAP8buARoBTaa2Y/dvbnca6nQikimxDhGOw54xd1fBTCzHwJTgeortHGsxtMXM5tVWGQ8CKHlhfAyh5YXlDlu5dScnrvBFCzs8f+rHtjX42etwJ/2J1PoY7SzSv+WqhJaXggvc2h5QZlT4+4L3X1Mj6PnXx69Fex+dZdDL7QiIklpBRp7vG8A9vfxe4tSoRUR6d1GYJSZnW1mxwFXAT/uz4VCvxlWlWNERYSWF8LLHFpeUOaq5O6dZvZF4GdADfCAu2/rz7US38pGROR3nYYOREQSpkIrIpKwIAttXI/FVYqZPWBmB81sa9pZojCzRjN7xsxazGybmc1OO1MpZjbYzDaY2S8KmeennSkKM6sxs5fMbGXaWaIwsz1m9ksz22Jmm9LOE4rgxmgLj8XtoMdjccDV/XksrlLM7ELgHeAhdz837TylmNkIYIS7v2hmJwGbgWlV3sYGDHX3d8ysFlgHzHb39SlHK8rMvgKMAU5290lp5ynFzPYAY9xde9yXIcQe7ZHH4tz9EHD4sbiq5e5rgTfSzhGVu7e7+4uF128DLXQ/JVO1vNs7hbe1haOqexFm1gB8GliUdhZJVoiFtrfH4qq6CITMzJqA0cALKUcpqfDP8C3AQeBJd6/2zHcBc4HqW6m6bw6sMbPNhcdXJYIQC21sj8VJcWZ2IvA4MMfd30o7Tynunnf3C+h+gmecmVXtMI2ZTQIOuvvmtLOUaby7/wnwF8D1hWExKSHEQhvbY3HSt8I45+PAw+7+o7TzlMPd3wSeBS5LN0lR44EphTHPHwJ/bmY/SDdSae6+v/DrQWA53UN5UkKIhTa2x+Kkd4UbS/cDLe5+Z9p5ojCzM8zs1MLrIcCngO2phirC3W9y9wZ3b6L7v+Gn3X1GyrGKMrOhhZujmNlQ4FIgiJk0aQuu0Lp7J3D4sbgWYFl/H4urFDNbAjwP/JGZtZrZzLQzlTAeuIbuXtaWwnF52qFKGAE8Y2Yv0/2X8ZPuHsSUqYCcBawzs18AG4BV7v7TlDMFIbjpXSIioQmuRysiEhoVWhGRhKnQiogkTIVWRCRhKrQiIglToRURSZgKrYhIwv4/6bwVXjbJVlQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ModelRe-evaluation\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "cm=confusion_matrix(ytest,ypred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "print('Accuracy:',accuracy_score(ytest,ypred))\n",
    "print(classification_report(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LogisticRegression in module sklearn.linear_model._logistic object:\n",
      "\n",
      "class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      " |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      " |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      " |      only supported by the 'saga' solver. If 'none' (not supported by the\n",
      " |      liblinear solver), no regularization is applied.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      " |      data. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem.\n",
      " |  \n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      " |        'saga' are faster for large ones.\n",
      " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      " |        schemes.\n",
      " |      - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
      " |      - 'liblinear' and 'saga' also handle L1 penalty\n",
      " |      - 'saga' also supports 'elasticnet' penalty\n",
      " |      - 'liblinear' does not support setting ``penalty='none'``\n",
      " |  \n",
      " |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |      features with approximately the same scale. You can\n",
      " |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  l1_ratio : float, default=None\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |      SAGA: A Fast Incremental Gradient Method With Support\n",
      " |      for Non-Strongly Convex Composite Objectives\n",
      " |      https://arxiv.org/abs/1407.0202\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTreeClassifier in module sklearn.tree._classes:\n",
      "\n",
      "class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n",
      " |  DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0)\n",
      " |  \n",
      " |  A decision tree classifier.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |  \n",
      " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |          - If int, then consider `max_features` features at each split.\n",
      " |          - If float, then `max_features` is a fraction and\n",
      " |            `int(max_features * n_features)` features are considered at each\n",
      " |            split.\n",
      " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |          - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the randomness of the estimator. The features are always\n",
      " |      randomly permuted at each split, even if ``splitter`` is set to\n",
      " |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      " |      select ``max_features`` at random at each split before finding the best\n",
      " |      split among them. But the best found split may vary across different\n",
      " |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      " |      improvement of the criterion is identical for several splits and one\n",
      " |      split has to be selected at random. To obtain a deterministic behaviour\n",
      " |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=0\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 1.0 (renaming of 0.25).\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  class_weight : dict, list of dict or \"balanced\", default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If None, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
      " |      The classes labels (single output problem),\n",
      " |      or a list of arrays of class labels (multi-output problem).\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance [4]_.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_classes_ : int or list of int\n",
      " |      The number of classes (for single output problems),\n",
      " |      or a list containing the number of classes for each\n",
      " |      output (for multi-output problems).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree instance\n",
      " |      The underlying Tree object. Please refer to\n",
      " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      " |      for basic usage of these attributes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor : A decision tree regressor.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
      " |  function on the outputs of :meth:`predict_proba`. This means that in\n",
      " |  case the highest predicted probabilities are tied, the classifier will\n",
      " |  predict the tied class with the lowest index in :term:`classes_`.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
      " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
      " |  >>> iris = load_iris()\n",
      " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      " |  ...                             # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseDecisionTree\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      " |      Build a decision tree classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      " |          This parameter is deprecated and has no effect.\n",
      " |          It will be removed in 1.1 (renaming of 0.26).\n",
      " |      \n",
      " |          .. deprecated :: 0.24\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : DecisionTreeClassifier\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities of the input samples X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X, check_input=True)\n",
      " |      Predict class probabilities of the input samples X.\n",
      " |      \n",
      " |      The predicted class probability is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Return the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples,)\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      " |      \n",
      " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      " |      process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      " |          Dictionary-like object, with the following attributes.\n",
      " |      \n",
      " |          ccp_alphas : ndarray\n",
      " |              Effective alphas of subtree during pruning.\n",
      " |      \n",
      " |          impurities : ndarray\n",
      " |              Sum of the impurities of the subtree leaves for the\n",
      " |              corresponding alpha value in ``ccp_alphas``.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator CSR matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  get_depth(self)\n",
      " |      Return the depth of the decision tree.\n",
      " |      \n",
      " |      The depth of a tree is the maximum distance between the root\n",
      " |      and any leaf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.max_depth : int\n",
      " |          The maximum depth of the tree.\n",
      " |  \n",
      " |  get_n_leaves(self)\n",
      " |      Return the number of leaves of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.n_leaves : int\n",
      " |          Number of leaves.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          Normalized total reduction of criteria by feature\n",
      " |          (Gini importance).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BaggingClassifier in module sklearn.ensemble._bagging:\n",
      "\n",
      "class BaggingClassifier(sklearn.base.ClassifierMixin, BaseBagging)\n",
      " |  BaggingClassifier(base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      " |  \n",
      " |  A Bagging classifier.\n",
      " |  \n",
      " |  A Bagging classifier is an ensemble meta-estimator that fits base\n",
      " |  classifiers each on random subsets of the original dataset and then\n",
      " |  aggregate their individual predictions (either by voting or by averaging)\n",
      " |  to form a final prediction. Such a meta-estimator can typically be used as\n",
      " |  a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      " |  tree), by introducing randomization into its construction procedure and\n",
      " |  then making an ensemble out of it.\n",
      " |  \n",
      " |  This algorithm encompasses several works from the literature. When random\n",
      " |  subsets of the dataset are drawn as random subsets of the samples, then\n",
      " |  this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      " |  replacement, then the method is known as Bagging [2]_. When random subsets\n",
      " |  of the dataset are drawn as random subsets of the features, then the method\n",
      " |  is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      " |  on subsets of both samples and features, then the method is known as\n",
      " |  Random Patches [4]_.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <bagging>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.15\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  base_estimator : object, default=None\n",
      " |      The base estimator to fit on random subsets of the dataset.\n",
      " |      If None, then the base estimator is a\n",
      " |      :class:`~sklearn.tree.DecisionTreeClassifier`.\n",
      " |  \n",
      " |  n_estimators : int, default=10\n",
      " |      The number of base estimators in the ensemble.\n",
      " |  \n",
      " |  max_samples : int or float, default=1.0\n",
      " |      The number of samples to draw from X to train each base estimator (with\n",
      " |      replacement by default, see `bootstrap` for more details).\n",
      " |  \n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples.\n",
      " |  \n",
      " |  max_features : int or float, default=1.0\n",
      " |      The number of features to draw from X to train each base estimator (\n",
      " |      without replacement by default, see `bootstrap_features` for more\n",
      " |      details).\n",
      " |  \n",
      " |      - If int, then draw `max_features` features.\n",
      " |      - If float, then draw `max_features * X.shape[1]` features.\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether samples are drawn with replacement. If False, sampling\n",
      " |      without replacement is performed.\n",
      " |  \n",
      " |  bootstrap_features : bool, default=False\n",
      " |      Whether features are drawn with replacement.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization error. Only available if bootstrap=True.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit\n",
      " |      a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* constructor parameter.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel for both :meth:`fit` and\n",
      " |      :meth:`predict`. ``None`` means 1 unless in a\n",
      " |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      " |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the random resampling of the original dataset\n",
      " |      (sample wise and feature wise).\n",
      " |      If the base estimator accepts a `random_state` attribute, a different\n",
      " |      seed is generated for each instance in the ensemble.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : estimator\n",
      " |      The base estimator from which the ensemble is grown.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when :meth:`fit` is performed.\n",
      " |  \n",
      " |  estimators_ : list of estimators\n",
      " |      The collection of fitted base estimators.\n",
      " |  \n",
      " |  estimators_samples_ : list of arrays\n",
      " |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      " |      estimator. Each subset is defined by an array of the indices selected.\n",
      " |  \n",
      " |  estimators_features_ : list of arrays\n",
      " |      The subset of drawn features for each base estimator.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.svm import SVC\n",
      " |  >>> from sklearn.ensemble import BaggingClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=100, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = BaggingClassifier(base_estimator=SVC(),\n",
      " |  ...                         n_estimators=10, random_state=0).fit(X, y)\n",
      " |  >>> clf.predict([[0, 0, 0, 0]])\n",
      " |  array([1])\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      " |         databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      " |  \n",
      " |  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      " |         1996.\n",
      " |  \n",
      " |  .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      " |         forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      " |         1998.\n",
      " |  \n",
      " |  .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      " |         Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BaggingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseBagging\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Average of the decision functions of the base classifiers.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray of shape (n_samples, k)\n",
      " |          The decision function of the input samples. The columns correspond\n",
      " |          to the classes in sorted order, as they appear in the attribute\n",
      " |          ``classes_``. Regression and binary classification are special\n",
      " |          cases with ``k == 1``, otherwise ``k==n_classes``.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed as the class with\n",
      " |      the highest mean predicted probability. If base estimators do not\n",
      " |      implement a ``predict_proba`` method, then it resorts to voting.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the base\n",
      " |      estimators in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the mean predicted class probabilities of the base estimators in the\n",
      " |      ensemble. If base estimators do not implement a ``predict_proba``\n",
      " |      method, then it resorts to voting and the predicted class probabilities\n",
      " |      of an input sample represents the proportion of estimators predicting\n",
      " |      each class.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseBagging:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a Bagging ensemble of estimators from the training\n",
      " |         set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted.\n",
      " |          Note that this is supported only if the base estimator supports\n",
      " |          sample weighting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseBagging:\n",
      " |  \n",
      " |  estimators_samples_\n",
      " |      The subset of drawn samples for each base estimator.\n",
      " |      \n",
      " |      Returns a dynamically generated list of indices identifying\n",
      " |      the samples used for fitting each member of the ensemble, i.e.,\n",
      " |      the in-bag samples.\n",
      " |      \n",
      " |      Note: the list is re-created at each call to the property in order\n",
      " |      to reduce the object memory footprint by not storing the sampling\n",
      " |      data. Thus fetching the property may be slower than expected.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(BaggingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AdaBoostClassifier in module sklearn.ensemble._weight_boosting:\n",
      "\n",
      "class AdaBoostClassifier(sklearn.base.ClassifierMixin, BaseWeightBoosting)\n",
      " |  AdaBoostClassifier(base_estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
      " |  \n",
      " |  An AdaBoost classifier.\n",
      " |  \n",
      " |  An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n",
      " |  classifier on the original dataset and then fits additional copies of the\n",
      " |  classifier on the same dataset but where the weights of incorrectly\n",
      " |  classified instances are adjusted such that subsequent classifiers focus\n",
      " |  more on difficult cases.\n",
      " |  \n",
      " |  This class implements the algorithm known as AdaBoost-SAMME [2].\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <adaboost>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.14\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  base_estimator : object, default=None\n",
      " |      The base estimator from which the boosted ensemble is built.\n",
      " |      Support for sample weighting is required, as well as proper\n",
      " |      ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n",
      " |      the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n",
      " |      initialized with `max_depth=1`.\n",
      " |  \n",
      " |  n_estimators : int, default=50\n",
      " |      The maximum number of estimators at which boosting is terminated.\n",
      " |      In case of perfect fit, the learning procedure is stopped early.\n",
      " |  \n",
      " |  learning_rate : float, default=1.\n",
      " |      Weight applied to each classifier at each boosting iteration. A higher\n",
      " |      learning rate increases the contribution of each classifier. There is\n",
      " |      a trade-off between the `learning_rate` and `n_estimators` parameters.\n",
      " |  \n",
      " |  algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n",
      " |      If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
      " |      ``base_estimator`` must support calculation of class probabilities.\n",
      " |      If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
      " |      The SAMME.R algorithm typically converges faster than SAMME,\n",
      " |      achieving a lower test error with fewer boosting iterations.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the random seed given at each `base_estimator` at each\n",
      " |      boosting iteration.\n",
      " |      Thus, it is only used when `base_estimator` exposes a `random_state`.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : estimator\n",
      " |      The base estimator from which the ensemble is grown.\n",
      " |  \n",
      " |  estimators_ : list of classifiers\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_classes_ : int\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  estimator_weights_ : ndarray of floats\n",
      " |      Weights for each estimator in the boosted ensemble.\n",
      " |  \n",
      " |  estimator_errors_ : ndarray of floats\n",
      " |      Classification error for each estimator in the boosted\n",
      " |      ensemble.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances if supported by the\n",
      " |      ``base_estimator`` (when based on decision trees).\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  AdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n",
      " |      regressor on the original dataset and then fits additional copies of\n",
      " |      the regressor on the same dataset but where the weights of instances\n",
      " |      are adjusted according to the error of the current prediction.\n",
      " |  \n",
      " |  GradientBoostingClassifier : GB builds an additive model in a forward\n",
      " |      stage-wise fashion. Regression trees are fit on the negative gradient\n",
      " |      of the binomial or multinomial deviance loss function. Binary\n",
      " |      classification is a special case where only a single regression tree is\n",
      " |      induced.\n",
      " |  \n",
      " |  sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n",
      " |      method used for classification.\n",
      " |      Creates a model that predicts the value of a target variable by\n",
      " |      learning simple decision rules inferred from the data features.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      " |         on-Line Learning and an Application to Boosting\", 1995.\n",
      " |  \n",
      " |  .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import AdaBoostClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      " |  >>> clf.predict([[0, 0, 0, 0]])\n",
      " |  array([1])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.983...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AdaBoostClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseWeightBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, base_estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray of shape of (n_samples, k)\n",
      " |          The decision function of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |          Binary classification is a special cases with ``k == 1``,\n",
      " |          otherwise ``k==n_classes``. For binary classification,\n",
      " |          values closer to -1 or 1 mean more like the first or second\n",
      " |          class in ``classes_``, respectively.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a boosted classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values (class labels).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, the sample weights are initialized to\n",
      " |          ``1 / n_samples``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict classes for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed as the weighted mean\n",
      " |      prediction of the classifiers in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the weighted mean predicted class log-probabilities of the classifiers\n",
      " |      in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the weighted mean predicted class probabilities of the classifiers\n",
      " |      in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |  \n",
      " |  staged_decision_function(self, X)\n",
      " |      Compute decision function of ``X`` for each boosting iteration.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each boosting iteration.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      score : generator of ndarray of shape (n_samples, k)\n",
      " |          The decision function of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |          Binary classification is a special cases with ``k == 1``,\n",
      " |          otherwise ``k==n_classes``. For binary classification,\n",
      " |          values closer to -1 or 1 mean more like the first or second\n",
      " |          class in ``classes_``, respectively.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Return staged predictions for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed as the weighted mean\n",
      " |      prediction of the classifiers in the ensemble.\n",
      " |      \n",
      " |      This generator method yields the ensemble prediction after each\n",
      " |      iteration of boosting and therefore allows monitoring, such as to\n",
      " |      determine the prediction on a test set after each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  staged_predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the weighted mean predicted class probabilities of the classifiers\n",
      " |      in the ensemble.\n",
      " |      \n",
      " |      This generator method yields the ensemble predicted class probabilities\n",
      " |      after each iteration of boosting and therefore allows monitoring, such\n",
      " |      as to determine the predicted class probabilities on a test set after\n",
      " |      each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Yields\n",
      " |      -------\n",
      " |      p : generator of ndarray of shape (n_samples,)\n",
      " |          The class probabilities of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseWeightBoosting:\n",
      " |  \n",
      " |  staged_score(self, X, y, sample_weight=None)\n",
      " |      Return staged scores for X, y.\n",
      " |      \n",
      " |      This generator method yields the ensemble score after each iteration of\n",
      " |      boosting and therefore allows monitoring, such as to determine the\n",
      " |      score on a test set after each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      z : float\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseWeightBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The feature importances.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(AdaBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GradientBoostingClassifier in module sklearn.ensemble._gb:\n",
      "\n",
      "class GradientBoostingClassifier(sklearn.base.ClassifierMixin, BaseGradientBoosting)\n",
      " |  GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |  \n",
      " |  Gradient Boosting for classification.\n",
      " |  \n",
      " |  GB builds an additive model in a\n",
      " |  forward stage-wise fashion; it allows for the optimization of\n",
      " |  arbitrary differentiable loss functions. In each stage ``n_classes_``\n",
      " |  regression trees are fit on the negative gradient of the\n",
      " |  binomial or multinomial deviance loss function. Binary classification\n",
      " |  is a special case where only a single regression tree is induced.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : {'deviance', 'exponential'}, default='deviance'\n",
      " |      The loss function to be optimized. 'deviance' refers to\n",
      " |      deviance (= logistic regression) for classification\n",
      " |      with probabilistic outputs. For loss 'exponential' gradient\n",
      " |      boosting recovers the AdaBoost algorithm.\n",
      " |  \n",
      " |  learning_rate : float, default=0.1\n",
      " |      Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      " |      There is a trade-off between learning_rate and n_estimators.\n",
      " |  \n",
      " |  n_estimators : int, default=100\n",
      " |      The number of boosting stages to perform. Gradient boosting\n",
      " |      is fairly robust to over-fitting so a large number usually\n",
      " |      results in better performance.\n",
      " |  \n",
      " |  subsample : float, default=1.0\n",
      " |      The fraction of samples to be used for fitting the individual base\n",
      " |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      " |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      " |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |  criterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are 'friedman_mse' for the mean squared error with improvement\n",
      " |      score by Friedman, 'mse' for mean squared error, and 'mae' for\n",
      " |      the mean absolute error. The default value of 'friedman_mse' is\n",
      " |      generally the best as it can provide a better approximation in\n",
      " |      some cases.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |      .. deprecated:: 0.24\n",
      " |          `criterion='mae'` is deprecated and will be removed in version\n",
      " |          1.1 (renaming of 0.26). Use `criterion='friedman_mse'` or `'mse'`\n",
      " |          instead, as trees should use a least-square criterion in\n",
      " |          Gradient Boosting.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_depth : int, default=3\n",
      " |      The maximum depth of the individual regression estimators. The maximum\n",
      " |      depth limits the number of nodes in the tree. Tune this parameter\n",
      " |      for best performance; the best value depends on the interaction\n",
      " |      of the input variables.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=None\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 1.0 (renaming of 0.25).\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  init : estimator or 'zero', default=None\n",
      " |      An estimator object that is used to compute the initial predictions.\n",
      " |      ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n",
      " |      'zero', the initial raw predictions are set to zero. By default, a\n",
      " |      ``DummyEstimator`` predicting the classes priors is used.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the random seed given to each Tree estimator at each\n",
      " |      boosting iteration.\n",
      " |      In addition, it controls the random permutation of the features at\n",
      " |      each split (see Notes for more details).\n",
      " |      It also controls the random spliting of the training data to obtain a\n",
      " |      validation set if `n_iter_no_change` is not None.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If 'auto', then `max_features=sqrt(n_features)`.\n",
      " |      - If 'sqrt', then `max_features=sqrt(n_features)`.\n",
      " |      - If 'log2', then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Enable verbose output. If 1 then it prints progress and performance\n",
      " |      once in a while (the more trees the lower the frequency). If greater\n",
      " |      than 1 then it prints progress and performance for every tree.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  n_iter_no_change : int, default=None\n",
      " |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      " |      to terminate training when validation score is not improving. By\n",
      " |      default it is set to None to disable early stopping. If set to a\n",
      " |      number, it will set aside ``validation_fraction`` size of the training\n",
      " |      data as validation and terminate training when validation score is not\n",
      " |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      " |      iterations. The split is stratified.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for the early stopping. When the loss is not improving\n",
      " |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      " |      number), the training stops.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_estimators_ : int\n",
      " |      The number of estimators as selected by early stopping (if\n",
      " |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      " |      ``n_estimators``.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_improvement_ : ndarray of shape (n_estimators,)\n",
      " |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      " |      relative to the previous iteration.\n",
      " |      ``oob_improvement_[0]`` is the improvement in\n",
      " |      loss of the first stage over the ``init`` estimator.\n",
      " |      Only available if ``subsample < 1.0``\n",
      " |  \n",
      " |  train_score_ : ndarray of shape (n_estimators,)\n",
      " |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      " |      model at iteration ``i`` on the in-bag sample.\n",
      " |      If ``subsample == 1`` this is the deviance on the training data.\n",
      " |  \n",
      " |  loss_ : LossFunction\n",
      " |      The concrete ``LossFunction`` object.\n",
      " |  \n",
      " |  init_ : estimator\n",
      " |      The estimator that provides the initial predictions.\n",
      " |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      " |  \n",
      " |  estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, ``loss_.K``)\n",
      " |      The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
      " |      classification, otherwise n_classes.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of data features.\n",
      " |  \n",
      " |  n_classes_ : int\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  HistGradientBoostingClassifier : Histogram-based Gradient Boosting\n",
      " |      Classification Tree.\n",
      " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      " |  RandomForestClassifier : A meta-estimator that fits a number of decision\n",
      " |      tree classifiers on various sub-samples of the dataset and uses\n",
      " |      averaging to improve the predictive accuracy and control over-fitting.\n",
      " |  AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n",
      " |      on the original dataset and then fits additional copies of the\n",
      " |      classifier on the same dataset where the weights of incorrectly\n",
      " |      classified instances are adjusted such that subsequent classifiers\n",
      " |      focus more on difficult cases.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data and\n",
      " |  ``max_features=n_features``, if the improvement of the criterion is\n",
      " |  identical for several splits enumerated during the search of the best\n",
      " |  split. To obtain a deterministic behaviour during fitting,\n",
      " |  ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      " |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      " |  \n",
      " |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      " |  \n",
      " |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      " |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  The following example shows how to fit a gradient boosting classifier with\n",
      " |  100 decision stumps as weak learners.\n",
      " |  \n",
      " |  >>> from sklearn.datasets import make_hastie_10_2\n",
      " |  >>> from sklearn.ensemble import GradientBoostingClassifier\n",
      " |  \n",
      " |  >>> X, y = make_hastie_10_2(random_state=0)\n",
      " |  >>> X_train, X_test = X[:2000], X[2000:]\n",
      " |  >>> y_train, y_test = y[:2000], y[2000:]\n",
      " |  \n",
      " |  >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
      " |  ...     max_depth=1, random_state=0).fit(X_train, y_train)\n",
      " |  >>> clf.score(X_test, y_test)\n",
      " |  0.913...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GradientBoostingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseGradientBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray of shape (n_samples, n_classes) or (n_samples,)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          order of the classes corresponds to that in the attribute\n",
      " |          :term:`classes_`. Regression and binary classification produce an\n",
      " |          array of shape (n_samples,).\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  staged_decision_function(self, X)\n",
      " |      Compute decision function of ``X`` for each iteration.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : generator of ndarray of shape (n_samples, k)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |          Regression and binary classification are special cases with\n",
      " |          ``k == 1``, otherwise ``k==n_classes``.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Predict class at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  staged_predict_proba(self, X)\n",
      " |      Predict class probabilities at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the ensemble to X, return leaf indices.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      " |          be converted to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples, n_estimators, n_classes)\n",
      " |          For each datapoint x in X and for each tree in the ensemble,\n",
      " |          return the index of the leaf x ends up in each estimator.\n",
      " |          In the case of binary classification n_classes is 1.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      " |      Fit the gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values (strings or integers in classification, real numbers\n",
      " |          in regression)\n",
      " |          For classification, labels must correspond to classes.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      monitor : callable, default=None\n",
      " |          The monitor is called after each iteration with the current\n",
      " |          iteration, a reference to the estimator and the local variables of\n",
      " |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      " |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      " |          is stopped. The monitor can be used for various things such as\n",
      " |          computing held-out estimates, early stopping, model introspect, and\n",
      " |          snapshoting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `round(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=None\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 1.0 (renaming of 0.25).\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      " |      Only available if bootstrap=True.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0, 1)`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeClassifier\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(...)\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest.\n",
      " |      The class probability of a single tree is the fraction of samples of\n",
      " |      the same class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBClassifier in module xgboost.sklearn:\n",
      "\n",
      "class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      " |  XGBClassifier(*, objective='binary:logistic', use_label_encoder=True, **kwargs)\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for XGBoost classification.\n",
      " |  \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |      n_estimators : int\n",
      " |          Number of boosting rounds.\n",
      " |      use_label_encoder : bool\n",
      " |          (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new\n",
      " |          code, we recommend that you set this parameter to False.\n",
      " |  \n",
      " |      max_depth : int\n",
      " |          Maximum tree depth for base learners.\n",
      " |      learning_rate : float\n",
      " |          Boosting learning rate (xgb's \"eta\")\n",
      " |      verbosity : int\n",
      " |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      " |      objective : string or callable\n",
      " |          Specify the learning task and the corresponding learning objective or\n",
      " |          a custom objective function to be used (see note below).\n",
      " |      booster: string\n",
      " |          Specify which booster to use: gbtree, gblinear or dart.\n",
      " |      tree_method: string\n",
      " |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      " |          is set to default, XGBoost will choose the most conservative option\n",
      " |          available.  It's recommended to study this option from parameters\n",
      " |          document.\n",
      " |      n_jobs : int\n",
      " |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      " |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      " |          balance the threads.  Creating thread contention will significantly slow down both\n",
      " |          algorithms.\n",
      " |      gamma : float\n",
      " |          Minimum loss reduction required to make a further partition on a leaf\n",
      " |          node of the tree.\n",
      " |      min_child_weight : float\n",
      " |          Minimum sum of instance weight(hessian) needed in a child.\n",
      " |      max_delta_step : float\n",
      " |          Maximum delta step we allow each tree's weight estimation to be.\n",
      " |      subsample : float\n",
      " |          Subsample ratio of the training instance.\n",
      " |      colsample_bytree : float\n",
      " |          Subsample ratio of columns when constructing each tree.\n",
      " |      colsample_bylevel : float\n",
      " |          Subsample ratio of columns for each level.\n",
      " |      colsample_bynode : float\n",
      " |          Subsample ratio of columns for each split.\n",
      " |      reg_alpha : float (xgb's alpha)\n",
      " |          L1 regularization term on weights\n",
      " |      reg_lambda : float (xgb's lambda)\n",
      " |          L2 regularization term on weights\n",
      " |      scale_pos_weight : float\n",
      " |          Balancing of positive and negative weights.\n",
      " |      base_score:\n",
      " |          The initial prediction score of all instances, global bias.\n",
      " |      random_state : int\n",
      " |          Random number seed.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      " |             it uses Hogwild algorithm.\n",
      " |  \n",
      " |      missing : float, default np.nan\n",
      " |          Value in the data which needs to be present as a missing value.\n",
      " |      num_parallel_tree: int\n",
      " |          Used for boosting random forest.\n",
      " |      monotone_constraints : str\n",
      " |          Constraint of variable monotonicity.  See tutorial for more\n",
      " |          information.\n",
      " |      interaction_constraints : str\n",
      " |          Constraints for interaction representing permitted interactions.  The\n",
      " |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      " |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      " |          that are allowed to interact with each other.  See tutorial for more\n",
      " |          information\n",
      " |      importance_type: string, default \"gain\"\n",
      " |          The feature importance type for the feature_importances\\_ property:\n",
      " |          either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      " |      gpu_id :\n",
      " |          Device ordinal.\n",
      " |      validate_parameters :\n",
      " |          Give warnings for unknown parameter.\n",
      " |  \n",
      " |      \\*\\*kwargs : dict, optional\n",
      " |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      " |          parameters can be found here:\n",
      " |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      " |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      " |          dict simultaneously will result in a TypeError.\n",
      " |  \n",
      " |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      " |  \n",
      " |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      " |              that parameters passed via this argument will interact properly\n",
      " |              with scikit-learn.\n",
      " |  \n",
      " |          .. note::  Custom objective function\n",
      " |  \n",
      " |              A custom objective function can be provided for the ``objective``\n",
      " |              parameter. In this case, it should have the signature\n",
      " |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |              y_true: array_like of shape [n_samples]\n",
      " |                  The target values\n",
      " |              y_pred: array_like of shape [n_samples]\n",
      " |                  The predicted values\n",
      " |  \n",
      " |              grad: array_like of shape [n_samples]\n",
      " |                  The value of the gradient for each sample point.\n",
      " |              hess: array_like of shape [n_samples]\n",
      " |                  The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBClassifier\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, objective='binary:logistic', use_label_encoder=True, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  evals_result(self)\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If **eval_set** is passed to the `fit` function, you can call\n",
      " |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      " |      When **eval_metric** is also passed to the `fit` function, the\n",
      " |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result : dictionary\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      " |      \n",
      " |          clf = xgb.XGBClassifier(**param_dist)\n",
      " |      \n",
      " |          clf.fit(X_train, y_train,\n",
      " |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      " |                  eval_metric='logloss',\n",
      " |                  verbose=True)\n",
      " |      \n",
      " |          evals_result = clf.evals_result()\n",
      " |      \n",
      " |      The variable **evals_result** will contain\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |  \n",
      " |  fit(self, X, y, *, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, base_margin_eval_set=None, feature_weights=None, callbacks=None)\n",
      " |      Fit gradient boosting classifier.\n",
      " |      \n",
      " |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      " |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      " |      pass ``xgb_model`` argument.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix\n",
      " |      y : array_like\n",
      " |          Labels\n",
      " |      sample_weight : array_like\n",
      " |          instance weights\n",
      " |      base_margin : array_like\n",
      " |          global bias for each instance.\n",
      " |      eval_set : list, optional\n",
      " |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      " |          metrics will be computed.\n",
      " |          Validation metrics will help us track the performance of the model.\n",
      " |      eval_metric : str, list of str, or callable, optional\n",
      " |          If a str, should be a built-in evaluation metric to use. See\n",
      " |          doc/parameter.rst.\n",
      " |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      " |          to use.\n",
      " |          If callable, a custom evaluation metric. The call signature is\n",
      " |          ``func(y_predicted, y_true)`` where ``y_true`` will be a DMatrix object such\n",
      " |          that you may need to call the ``get_label`` method. It must return a str,\n",
      " |          value pair where the str is a name for the evaluation and value is the value\n",
      " |          of the evaluation function. The callable custom objective is always minimized.\n",
      " |      early_stopping_rounds : int\n",
      " |          Activates early stopping. Validation metric needs to improve at least once in\n",
      " |          every **early_stopping_rounds** round(s) to continue training.\n",
      " |          Requires at least one item in **eval_set**.\n",
      " |          The method returns the model from the last iteration (not the best one).\n",
      " |          If there's more than one item in **eval_set**, the last entry will be used\n",
      " |          for early stopping.\n",
      " |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      " |          used for early stopping.\n",
      " |          If early stopping occurs, the model will have three additional fields:\n",
      " |          ``clf.best_score``, ``clf.best_iteration`` and ``clf.best_ntree_limit``.\n",
      " |      verbose : bool\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation metric\n",
      " |          measured on the validation set to stderr.\n",
      " |      xgb_model :\n",
      " |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      " |          loaded before training (allows training continuation).\n",
      " |      sample_weight_eval_set : list, optional\n",
      " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
      " |          object storing instance weights for the i-th validation set.\n",
      " |      base_margin_eval_set : list, optional\n",
      " |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      " |          object storing base margin for the i-th validation set.\n",
      " |      feature_weights: array_like\n",
      " |          Weight for each feature, defines the probability of each feature being\n",
      " |          selected when colsample is being used.  All values must be greater than 0,\n",
      " |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      " |          `exact` tree methods.\n",
      " |      callbacks : list of callback functions\n",
      " |          List of callback functions that are applied at end of each iteration.\n",
      " |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      " |          Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      " |                                                      save_best=True)]\n",
      " |  \n",
      " |  predict(self, X, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None, iteration_range: Union[Tuple[int, int], NoneType] = None)\n",
      " |      Predict with `X`.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Data to predict with\n",
      " |      output_margin : bool\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      ntree_limit : int\n",
      " |          Deprecated, use `iteration_range` instead.\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin : array_like\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
      " |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
      " |          used in this prediction.\n",
      " |      \n",
      " |          .. versionadded:: 1.4.0\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |  \n",
      " |  predict_proba(self, X, ntree_limit=None, validate_features=False, base_margin=None, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n",
      " |      Predict the probability of each `X` example being of a given class.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix.\n",
      " |      ntree_limit : int\n",
      " |          Deprecated, use `iteration_range` instead.\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin : array_like\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
      " |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
      " |          used in this prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n",
      " |          probability of each data example being of a given class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  apply(self, X, ntree_limit: int = 0, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n",
      " |      Return the predicted leaf every tree for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  get_booster(self)\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_num_boosting_rounds(self)\n",
      " |      Gets the number of xgboost boosting rounds.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters.\n",
      " |  \n",
      " |  get_xgb_params(self)\n",
      " |      Get xgboost specific parameters.\n",
      " |  \n",
      " |  load_model(self, fname)\n",
      " |      Load the model from a file.\n",
      " |      \n",
      " |      The model is loaded from an XGBoost internal format which is universal\n",
      " |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      " |      Python Booster object (such as feature names) will not be loaded.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Input file name.\n",
      " |  \n",
      " |  save_model(self, fname: str)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal format which is universal\n",
      " |      among the various XGBoost interfaces. Auxiliary attributes of the\n",
      " |      Python Booster object (such as feature names) will not be saved.\n",
      " |      \n",
      " |        .. note::\n",
      " |      \n",
      " |          See:\n",
      " |      \n",
      " |          https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Output file name\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      " |      allow unknown kwargs. This allows using the full range of xgboost\n",
      " |      parameters that are not defined as member variables in sklearn grid\n",
      " |      search.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from XGBModel:\n",
      " |  \n",
      " |  best_iteration\n",
      " |  \n",
      " |  best_ntree_limit\n",
      " |  \n",
      " |  best_score\n",
      " |  \n",
      " |  coef_\n",
      " |      Coefficients property\n",
      " |      \n",
      " |      .. note:: Coefficients are defined only for linear learners\n",
      " |      \n",
      " |          Coefficients are only defined when the linear model is chosen as\n",
      " |          base learner (`booster=gblinear`). It is not defined for other base\n",
      " |          learner types, such as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Feature importances property\n",
      " |      \n",
      " |      .. note:: Feature importance is defined only for tree boosters\n",
      " |      \n",
      " |          Feature importance is only defined when the decision tree model is chosen as base\n",
      " |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      " |          as linear learners (`booster=gblinear`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape ``[n_features]``\n",
      " |  \n",
      " |  intercept_\n",
      " |      Intercept (bias) property\n",
      " |      \n",
      " |      .. note:: Intercept is defined only for linear learners\n",
      " |      \n",
      " |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      " |          as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      " |  \n",
      " |  n_features_in_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KNeighborsClassifier in module sklearn.neighbors._classification:\n",
      "\n",
      "class KNeighborsClassifier(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      " |  KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      " |  \n",
      " |  Classifier implementing the k-nearest neighbors vote.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, default=5\n",
      " |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      " |  \n",
      " |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      " |      weight function used in prediction.  Possible values:\n",
      " |  \n",
      " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      " |        are weighted equally.\n",
      " |      - 'distance' : weight points by the inverse of their distance.\n",
      " |        in this case, closer neighbors of a query point will have a\n",
      " |        greater influence than neighbors which are further away.\n",
      " |      - [callable] : a user-defined function which accepts an\n",
      " |        array of distances, and returns an array of the same shape\n",
      " |        containing the weights.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDTree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, default=30\n",
      " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      " |      speed of the construction and query, as well as the memory\n",
      " |      required to store the tree.  The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  p : int, default=2\n",
      " |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric : str or callable, default='minkowski'\n",
      " |      the distance metric to use for the tree.  The default metric is\n",
      " |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      " |      metric. See the documentation of :class:`DistanceMetric` for a\n",
      " |      list of available metrics.\n",
      " |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      " |      must be square during fit. X may be a :term:`sparse graph`,\n",
      " |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      " |  \n",
      " |  metric_params : dict, default=None\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |      Doesn't affect :meth:`fit` method.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : array of shape (n_classes,)\n",
      " |      Class labels known to the classifier\n",
      " |  \n",
      " |  effective_metric_ : str or callble\n",
      " |      The distance metric used. It will be same as the `metric` parameter\n",
      " |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      " |      'minkowski' and `p` parameter set to 2.\n",
      " |  \n",
      " |  effective_metric_params_ : dict\n",
      " |      Additional keyword arguments for the metric function. For most metrics\n",
      " |      will be same with `metric_params` parameter, but may also contain the\n",
      " |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      " |      'minkowski'.\n",
      " |  \n",
      " |  n_samples_fit_ : int\n",
      " |      Number of samples in the fitted data.\n",
      " |  \n",
      " |  outputs_2d_ : bool\n",
      " |      False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
      " |      otherwise True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[0], [1], [2], [3]]\n",
      " |  >>> y = [0, 0, 1, 1]\n",
      " |  >>> from sklearn.neighbors import KNeighborsClassifier\n",
      " |  >>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
      " |  >>> neigh.fit(X, y)\n",
      " |  KNeighborsClassifier(...)\n",
      " |  >>> print(neigh.predict([[1.1]]))\n",
      " |  [0]\n",
      " |  >>> print(neigh.predict_proba([[0.9]]))\n",
      " |  [[0.66666667 0.33333333]]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  RadiusNeighborsClassifier\n",
      " |  KNeighborsRegressor\n",
      " |  RadiusNeighborsRegressor\n",
      " |  NearestNeighbors\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      " |     neighbors, neighbor `k+1` and `k`, have identical distances\n",
      " |     but different labels, the results will depend on the ordering of the\n",
      " |     training data.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KNeighborsClassifier\n",
      " |      sklearn.neighbors._base.KNeighborsMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.neighbors._base.NeighborsBase\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the k-nearest neighbors classifier from the training dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : KNeighborsClassifier\n",
      " |          The fitted k-nearest neighbors classifier.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the class labels for the provided data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n",
      " |          Class labels for each data sample.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test data X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_queries, n_classes), or a list of n_outputs\n",
      " |          of such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. Classes are ordered\n",
      " |          by lexicographic order.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Finds the K-neighbors of a point.\n",
      " |      \n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int, default=None\n",
      " |          Number of neighbors required for each sample. The default is the\n",
      " |          value passed to the constructor.\n",
      " |      \n",
      " |      return_distance : bool, default=True\n",
      " |          Whether or not to return the distances.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True\n",
      " |      \n",
      " |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NearestNeighbors\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples)\n",
      " |      NearestNeighbors(n_neighbors=1)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      " |      (array([[0.5]]), array([[2]]))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False)\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |          For ``metric='precomputed'`` the shape should be\n",
      " |          (n_queries, n_indexed). Otherwise the shape should be\n",
      " |          (n_queries, n_features).\n",
      " |      \n",
      " |      n_neighbors : int, default=None\n",
      " |          Number of neighbors for each sample. The default is the value\n",
      " |          passed to the constructor.\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are Euclidean distance between points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      " |          `n_samples_fit` is the number of samples in the fitted data\n",
      " |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      " |          The matrix is of CSR format.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X)\n",
      " |      NearestNeighbors(n_neighbors=2)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[1., 0., 1.],\n",
      " |             [0., 1., 1.],\n",
      " |             [1., 0., 1.]])\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GaussianNB in module sklearn.naive_bayes:\n",
      "\n",
      "class GaussianNB(_BaseNB)\n",
      " |  GaussianNB(*, priors=None, var_smoothing=1e-09)\n",
      " |  \n",
      " |  Gaussian Naive Bayes (GaussianNB)\n",
      " |  \n",
      " |  Can perform online updates to model parameters via :meth:`partial_fit`.\n",
      " |  For details on algorithm used to update feature means and variance online,\n",
      " |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
      " |  \n",
      " |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  priors : array-like of shape (n_classes,)\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  var_smoothing : float, default=1e-9\n",
      " |      Portion of the largest variance of all features that is added to\n",
      " |      variances for calculation stability.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_count_ : ndarray of shape (n_classes,)\n",
      " |      number of training samples observed in each class.\n",
      " |  \n",
      " |  class_prior_ : ndarray of shape (n_classes,)\n",
      " |      probability of each class.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      class labels known to the classifier\n",
      " |  \n",
      " |  epsilon_ : float\n",
      " |      absolute additive value to variances\n",
      " |  \n",
      " |  sigma_ : ndarray of shape (n_classes, n_features)\n",
      " |      variance of each feature per class\n",
      " |  \n",
      " |  theta_ : ndarray of shape (n_classes, n_features)\n",
      " |      mean of each feature per class\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
      " |  >>> from sklearn.naive_bayes import GaussianNB\n",
      " |  >>> clf = GaussianNB()\n",
      " |  >>> clf.fit(X, Y)\n",
      " |  GaussianNB()\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  >>> clf_pf = GaussianNB()\n",
      " |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
      " |  GaussianNB()\n",
      " |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GaussianNB\n",
      " |      _BaseNB\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, priors=None, var_smoothing=1e-09)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Gaussian Naive Bayes according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance and numerical stability overhead,\n",
      " |      hence it is better to call partial_fit on chunks of data that are\n",
      " |      as large as possible (as long as fitting in the memory budget) to\n",
      " |      hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like of shape (n_classes,), default=None\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : ndarray of shape (n_samples,)\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MultinomialNB in module sklearn.naive_bayes:\n",
      "\n",
      "class MultinomialNB(_BaseDiscreteNB)\n",
      " |  MultinomialNB(*, alpha=1.0, fit_prior=True, class_prior=None)\n",
      " |  \n",
      " |  Naive Bayes classifier for multinomial models\n",
      " |  \n",
      " |  The multinomial Naive Bayes classifier is suitable for classification with\n",
      " |  discrete features (e.g., word counts for text classification). The\n",
      " |  multinomial distribution normally requires integer feature counts. However,\n",
      " |  in practice, fractional counts such as tf-idf may also work.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : float, default=1.0\n",
      " |      Additive (Laplace/Lidstone) smoothing parameter\n",
      " |      (0 for no smoothing).\n",
      " |  \n",
      " |  fit_prior : bool, default=True\n",
      " |      Whether to learn class prior probabilities or not.\n",
      " |      If false, a uniform prior will be used.\n",
      " |  \n",
      " |  class_prior : array-like of shape (n_classes,), default=None\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_count_ : ndarray of shape (n_classes,)\n",
      " |      Number of samples encountered for each class during fitting. This\n",
      " |      value is weighted by the sample weight when provided.\n",
      " |  \n",
      " |  class_log_prior_ : ndarray of shape (n_classes, )\n",
      " |      Smoothed empirical log probability for each class.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      Class labels known to the classifier\n",
      " |  \n",
      " |  coef_ : ndarray of shape (n_classes, n_features)\n",
      " |      Mirrors ``feature_log_prob_`` for interpreting `MultinomialNB`\n",
      " |      as a linear model.\n",
      " |  \n",
      " |      .. deprecated:: 0.24\n",
      " |          ``coef_`` is deprecated in 0.24 and will be removed in 1.1\n",
      " |          (renaming of 0.26).\n",
      " |  \n",
      " |  feature_count_ : ndarray of shape (n_classes, n_features)\n",
      " |      Number of samples encountered for each (class, feature)\n",
      " |      during fitting. This value is weighted by the sample weight when\n",
      " |      provided.\n",
      " |  \n",
      " |  feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
      " |      Empirical log probability of features\n",
      " |      given a class, ``P(x_i|y)``.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (n_classes,)\n",
      " |      Mirrors ``class_log_prior_`` for interpreting `MultinomialNB`\n",
      " |      as a linear model.\n",
      " |  \n",
      " |      .. deprecated:: 0.24\n",
      " |          ``intercept_`` is deprecated in 0.24 and will be removed in 1.1\n",
      " |          (renaming of 0.26).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      Number of features of each sample.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> rng = np.random.RandomState(1)\n",
      " |  >>> X = rng.randint(5, size=(6, 100))\n",
      " |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      " |  >>> from sklearn.naive_bayes import MultinomialNB\n",
      " |  >>> clf = MultinomialNB()\n",
      " |  >>> clf.fit(X, y)\n",
      " |  MultinomialNB()\n",
      " |  >>> print(clf.predict(X[2:3]))\n",
      " |  [3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  For the rationale behind the names `coef_` and `intercept_`, i.e.\n",
      " |  naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n",
      " |  Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
      " |  Information Retrieval. Cambridge University Press, pp. 234-265.\n",
      " |  https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MultinomialNB\n",
      " |      _BaseDiscreteNB\n",
      " |      _BaseNB\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseDiscreteNB:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Naive Bayes classifier according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance overhead hence it is better to call\n",
      " |      partial_fit on chunks of data that are as large as possible\n",
      " |      (as long as fitting in the memory budget) to hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like of shape (n_classes), default=None\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _BaseDiscreteNB:\n",
      " |  \n",
      " |  coef_\n",
      " |  \n",
      " |  intercept_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : ndarray of shape (n_samples,)\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SVC in module sklearn.svm._classes:\n",
      "\n",
      "class SVC(sklearn.svm._base.BaseSVC)\n",
      " |  SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
      " |  \n",
      " |  C-Support Vector Classification.\n",
      " |  \n",
      " |  The implementation is based on libsvm. The fit time scales at least\n",
      " |  quadratically with the number of samples and may be impractical\n",
      " |  beyond tens of thousands of samples. For large datasets\n",
      " |  consider using :class:`~sklearn.svm.LinearSVC` or\n",
      " |  :class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n",
      " |  :class:`~sklearn.kernel_approximation.Nystroem` transformer.\n",
      " |  \n",
      " |  The multiclass support is handled according to a one-vs-one scheme.\n",
      " |  \n",
      " |  For details on the precise mathematical formulation of the provided\n",
      " |  kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
      " |  other, see the corresponding section in the narrative documentation:\n",
      " |  :ref:`svm_kernels`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  C : float, default=1.0\n",
      " |      Regularization parameter. The strength of the regularization is\n",
      " |      inversely proportional to C. Must be strictly positive. The penalty\n",
      " |      is a squared l2 penalty.\n",
      " |  \n",
      " |  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n",
      " |      Specifies the kernel type to be used in the algorithm.\n",
      " |      It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      " |      a callable.\n",
      " |      If none is given, 'rbf' will be used. If a callable is given it is\n",
      " |      used to pre-compute the kernel matrix from data matrices; that matrix\n",
      " |      should be an array of shape ``(n_samples, n_samples)``.\n",
      " |  \n",
      " |  degree : int, default=3\n",
      " |      Degree of the polynomial kernel function ('poly').\n",
      " |      Ignored by all other kernels.\n",
      " |  \n",
      " |  gamma : {'scale', 'auto'} or float, default='scale'\n",
      " |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      " |  \n",
      " |      - if ``gamma='scale'`` (default) is passed then it uses\n",
      " |        1 / (n_features * X.var()) as value of gamma,\n",
      " |      - if 'auto', uses 1 / n_features.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
      " |  \n",
      " |  coef0 : float, default=0.0\n",
      " |      Independent term in kernel function.\n",
      " |      It is only significant in 'poly' and 'sigmoid'.\n",
      " |  \n",
      " |  shrinking : bool, default=True\n",
      " |      Whether to use the shrinking heuristic.\n",
      " |      See the :ref:`User Guide <shrinking_svm>`.\n",
      " |  \n",
      " |  probability : bool, default=False\n",
      " |      Whether to enable probability estimates. This must be enabled prior\n",
      " |      to calling `fit`, will slow down that method as it internally uses\n",
      " |      5-fold cross-validation, and `predict_proba` may be inconsistent with\n",
      " |      `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n",
      " |  \n",
      " |  tol : float, default=1e-3\n",
      " |      Tolerance for stopping criterion.\n",
      " |  \n",
      " |  cache_size : float, default=200\n",
      " |      Specify the size of the kernel cache (in MB).\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Set the parameter C of class i to class_weight[i]*C for\n",
      " |      SVC. If not given, all classes are supposed to have\n",
      " |      weight one.\n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |  verbose : bool, default=False\n",
      " |      Enable verbose output. Note that this setting takes advantage of a\n",
      " |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      " |      properly in a multithreaded context.\n",
      " |  \n",
      " |  max_iter : int, default=-1\n",
      " |      Hard limit on iterations within solver, or -1 for no limit.\n",
      " |  \n",
      " |  decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n",
      " |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
      " |      (n_samples, n_classes) as all other classifiers, or the original\n",
      " |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
      " |      (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n",
      " |      ('ovo') is always used as multi-class strategy. The parameter is\n",
      " |      ignored for binary classification.\n",
      " |  \n",
      " |      .. versionchanged:: 0.19\n",
      " |          decision_function_shape is 'ovr' by default.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *decision_function_shape='ovr'* is recommended.\n",
      " |  \n",
      " |      .. versionchanged:: 0.17\n",
      " |         Deprecated *decision_function_shape='ovo' and None*.\n",
      " |  \n",
      " |  break_ties : bool, default=False\n",
      " |      If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n",
      " |      :term:`predict` will break ties according to the confidence values of\n",
      " |      :term:`decision_function`; otherwise the first class among the tied\n",
      " |      classes is returned. Please note that breaking ties comes at a\n",
      " |      relatively high computational cost compared to a simple predict.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the pseudo random number generation for shuffling the data for\n",
      " |      probability estimates. Ignored when `probability` is False.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_weight_ : ndarray of shape (n_classes,)\n",
      " |      Multipliers of parameter C for each class.\n",
      " |      Computed based on the ``class_weight`` parameter.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n",
      " |      Weights assigned to the features (coefficients in the primal\n",
      " |      problem). This is only available in the case of a linear kernel.\n",
      " |  \n",
      " |      `coef_` is a readonly property derived from `dual_coef_` and\n",
      " |      `support_vectors_`.\n",
      " |  \n",
      " |  dual_coef_ : ndarray of shape (n_classes -1, n_SV)\n",
      " |      Dual coefficients of the support vector in the decision\n",
      " |      function (see :ref:`sgd_mathematical_formulation`), multiplied by\n",
      " |      their targets.\n",
      " |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
      " |      The layout of the coefficients in the multiclass case is somewhat\n",
      " |      non-trivial. See the :ref:`multi-class section of the User Guide\n",
      " |      <svm_multi_class>` for details.\n",
      " |  \n",
      " |  fit_status_ : int\n",
      " |      0 if correctly fitted, 1 otherwise (will raise warning)\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  support_ : ndarray of shape (n_SV)\n",
      " |      Indices of support vectors.\n",
      " |  \n",
      " |  support_vectors_ : ndarray of shape (n_SV, n_features)\n",
      " |      Support vectors.\n",
      " |  \n",
      " |  n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
      " |      Number of support vectors for each class.\n",
      " |  \n",
      " |  probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
      " |  probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
      " |      If `probability=True`, it corresponds to the parameters learned in\n",
      " |      Platt scaling to produce probability estimates from decision values.\n",
      " |      If `probability=False`, it's an empty array. Platt scaling uses the\n",
      " |      logistic function\n",
      " |      ``1 / (1 + exp(decision_value * probA_ + probB_))``\n",
      " |      where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n",
      " |      more information on the multiclass case and training procedure see\n",
      " |      section 8 of [1]_.\n",
      " |  \n",
      " |  shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
      " |      Array dimensions of training vector ``X``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.pipeline import make_pipeline\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      " |  >>> y = np.array([1, 1, 2, 2])\n",
      " |  >>> from sklearn.svm import SVC\n",
      " |  >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
      " |  >>> clf.fit(X, y)\n",
      " |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      " |                  ('svc', SVC(gamma='auto'))])\n",
      " |  \n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SVR : Support Vector Machine for Regression implemented using libsvm.\n",
      " |  \n",
      " |  LinearSVC : Scalable Linear Support Vector Machine for classification\n",
      " |      implemented using liblinear. Check the See Also section of\n",
      " |      LinearSVC for more comparison element.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] `LIBSVM: A Library for Support Vector Machines\n",
      " |      <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
      " |  \n",
      " |  .. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n",
      " |      machines and comparison to regularizedlikelihood methods.\"\n",
      " |      <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SVC\n",
      " |      sklearn.svm._base.BaseSVC\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.svm._base.BaseLibSVM\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.svm._base.BaseSVC:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Evaluates the decision function for the samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)\n",
      " |          Returns the decision function of the sample for each class\n",
      " |          in the model.\n",
      " |          If decision_function_shape='ovr', the shape is (n_samples,\n",
      " |          n_classes).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If decision_function_shape='ovo', the function values are proportional\n",
      " |      to the distance of the samples X to the separating hyperplane. If the\n",
      " |      exact distances are required, divide the function values by the norm of\n",
      " |      the weight vector (``coef_``). See also `this question\n",
      " |      <https://stats.stackexchange.com/questions/14876/\n",
      " |      interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n",
      " |      If decision_function_shape='ovr', the decision function is a monotonic\n",
      " |      transformation of ovo decision function.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on samples in X.\n",
      " |      \n",
      " |      For an one-class model, +1 or -1 is returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          (n_samples_test, n_samples_train).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,)\n",
      " |          Class labels for samples in X.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.svm._base.BaseSVC:\n",
      " |  \n",
      " |  predict_log_proba\n",
      " |      Compute log probabilities of possible outcomes for samples in X.\n",
      " |      \n",
      " |      The model need to have probability information computed at training\n",
      " |      time: fit with attribute `probability` set to True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          (n_samples_test, n_samples_train).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : ndarray of shape (n_samples, n_classes)\n",
      " |          Returns the log-probabilities of the sample for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The probability model is created using cross validation, so\n",
      " |      the results can be slightly different than those obtained by\n",
      " |      predict. Also, it will produce meaningless results on very small\n",
      " |      datasets.\n",
      " |  \n",
      " |  predict_proba\n",
      " |      Compute probabilities of possible outcomes for samples in X.\n",
      " |      \n",
      " |      The model need to have probability information computed at training\n",
      " |      time: fit with attribute `probability` set to True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          (n_samples_test, n_samples_train).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : ndarray of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The probability model is created using cross validation, so\n",
      " |      the results can be slightly different than those obtained by\n",
      " |      predict. Also, it will produce meaningless results on very small\n",
      " |      datasets.\n",
      " |  \n",
      " |  probA_\n",
      " |  \n",
      " |  probB_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.svm._base.BaseLibSVM:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the SVM model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)                 or (n_samples, n_samples)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          (n_samples, n_samples).\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Per-sample weights. Rescale C per sample. Higher weights\n",
      " |          force the classifier to put more emphasis on these points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      " |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      " |      \n",
      " |      If X is a dense array, then the other methods will not support sparse\n",
      " |      matrices as input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.svm._base.BaseLibSVM:\n",
      " |  \n",
      " |  coef_\n",
      " |  \n",
      " |  n_support_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Except LogisticRegression no other classifier model have multi class parameter. that why not perform any other classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
